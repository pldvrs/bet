#!/usr/bin/env python3
"""
01_ingest_data.py â€” L'Aspirateur (Architecture ETL Sniper V1 Pro)
==================================================================
Met Ã  jour games_history (scores d'hier + calendrier J+3), box_scores, et les cotes (Odds).
Aucun appel API dans le front-end : toute la donnÃ©e est ingÃ©rÃ©e ici.

Sources rÃ©utilisÃ©es :
  - fill_history.py â†’ workflow daily (ingest_recent_games, fetch_future_games, update_team_archetypes)
  - backend_engine.py â†’ ingestion API Basketball + Supabase
  - fetch_historical_odds.py / app_sniper_v27.py â†’ rÃ©cupÃ©ration cotes (home/away)

Usage:
  python 01_ingest_data.py              # mode daily (veille + J+3 + cotes)
  python 01_ingest_data.py --days 5     # Deep Fetch : 5 derniers jours (rÃ©sultats FT + upsert scores)
  python 01_ingest_data.py --no-odds    # sans rÃ©cupÃ©ration des cotes
  python 01_ingest_data.py --days-past 2 --days-future 5
  python 01_ingest_data.py --init-leagues 121,16   # Backfill EuroCup + BCL (2023-2024, 2024-2025)
"""

import argparse
import os
import sys
import time
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import requests
from dotenv import load_dotenv

_env_path = Path(__file__).resolve().parent / ".env"
load_dotenv(dotenv_path=_env_path, override=True)

# Connexion Supabase (rÃ©utilisation database.py)
from database import get_client

# -----------------------------------------------------------------------------
# CONFIG API (alignÃ©e app_sniper_v27 / fetch_historical_odds)
# -----------------------------------------------------------------------------
BASE_URL = "https://v1.basketball.api-sports.io"
BOOKMAKER_IDS = [17, 7, 1]  # Betclic, Unibet, Bwin
BET_HOME_AWAY_ID = 2
API_RETRIES = 3
API_RETRY_DELAY = 1.0
ODDS_SLEEP = 0.6  # rate limit entre deux appels odds


def _api_get(endpoint: str, params: Optional[Dict[str, Any]] = None) -> Tuple[Optional[dict], Optional[str]]:
    """Appel API Basketball (sans Streamlit). Retourne (data, err)."""
    api_key = (os.environ.get("API_BASKETBALL_KEY") or "").strip()
    if not api_key:
        return None, "API key manquante"
    url = f"{BASE_URL}/{endpoint}"
    headers = {"x-apisports-key": api_key}
    last_err: Optional[str] = None
    for attempt in range(API_RETRIES):
        try:
            r = requests.get(url, headers=headers, params=params or {}, timeout=15)
            data = r.json() if r.text else {}
            if r.status_code != 200:
                last_err = str(data.get("errors") or f"HTTP {r.status_code}")
                if r.status_code == 429:
                    time.sleep(API_RETRY_DELAY * (attempt + 1))
                continue
            if data.get("errors"):
                last_err = str(data["errors"])
                continue
            return data, None
        except Exception as e:
            last_err = str(e)
            time.sleep(API_RETRY_DELAY * (attempt + 1))
    return None, last_err


def fetch_odds_moneyline(game_id: int, league_id: int, season: str) -> Tuple[Optional[float], Optional[float]]:
    """
    Cotes Home/Away pour un match. PrioritÃ© Betclic (17), Unibet (7), Bwin (1).
    Retourne (home_odd, away_odd) ou (None, None).
    """
    data, err = _api_get("odds", {"game": game_id, "league": league_id, "season": season})
    if err or not data:
        return None, None
    resp = data.get("response")
    if not isinstance(resp, list) or len(resp) == 0:
        return None, None
    item = resp[0]
    for bm in (item.get("bookmakers") or []):
        if bm.get("id") not in BOOKMAKER_IDS:
            continue
        for bet in (bm.get("bets") or []):
            if int(bet.get("id", 0)) != BET_HOME_AWAY_ID:
                continue
            odd_h, odd_a = None, None
            for v in (bet.get("values") or []):
                val = (v.get("value") or "").strip().lower()
                try:
                    odd_f = float(v.get("odd") or 0)
                    if val == "home":
                        odd_h = odd_f
                    elif val == "away":
                        odd_a = odd_f
                except (TypeError, ValueError):
                    pass
            if odd_h is not None and odd_a is not None:
                return odd_h, odd_a
    for bm in (item.get("bookmakers") or []):
        for bet in (bm.get("bets") or []):
            if int(bet.get("id", 0)) != BET_HOME_AWAY_ID:
                continue
            odd_h, odd_a = None, None
            for v in (bet.get("values") or []):
                val = (v.get("value") or "").strip().lower()
                try:
                    odd_f = float(v.get("odd") or 0)
                    if val == "home":
                        odd_h = odd_f
                    elif val == "away":
                        odd_a = odd_f
                except (TypeError, ValueError):
                    pass
            if odd_h is not None and odd_a is not None:
                return odd_h, odd_a
    return None, None


def update_future_games_odds(supabase, max_games: int = 200) -> int:
    """
    Pour tous les matchs Ã  venir (home_score IS NULL) dans games_history,
    rÃ©cupÃ¨re les cotes via l'API et met Ã  jour home_odd, away_odd.
    Retourne le nombre de matchs mis Ã  jour.
    """
    if not supabase:
        return 0
    try:
        r = (
            supabase.table("games_history")
            .select("game_id, league_id, season")
            .is_("home_score", "null")
            .order("date", desc=False)
            .limit(max_games)
            .execute()
        )
        rows = r.data or []
    except Exception as e:
        print(f"   âš ï¸ Lecture games_history: {e}")
        return 0

    updated = 0
    for row in rows:
        gid = row.get("game_id")
        league_id = row.get("league_id")
        season = row.get("season") or ""
        if not gid or league_id is None:
            continue
        oh, oa = fetch_odds_moneyline(gid, league_id, season)
        time.sleep(ODDS_SLEEP)
        if oh is None and oa is None:
            continue
        try:
            supabase.table("games_history").update({
                "home_odd": oh,
                "away_odd": oa,
            }).eq("game_id", gid).execute()
            updated += 1
        except Exception as e:
            # Colonnes home_odd/away_odd absentes si migration non exÃ©cutÃ©e
            if "home_odd" in str(e) or "PGRST204" in str(e):
                print("   âš ï¸ ExÃ©cute schema_migration_odds.sql pour activer home_odd/away_odd.")
            break
    return updated


# -----------------------------------------------------------------------------
# PIPELINE PRINCIPAL
# -----------------------------------------------------------------------------


def run_ingest(
    days_past: int = 1,
    days_future: int = 3,
    max_games_past: int = 50,
    max_games_future: int = 150,
    fetch_odds: bool = True,
    skip_archetypes: bool = False,
) -> None:
    """
    ExÃ©cute le pipeline d'ingestion :
    1. RÃ©sultats de la veille (et J-2 si days_past > 1) + box_scores
    2. Calendrier des N prochains jours (matchs Ã  venir)
    3. Mise Ã  jour des archÃ©types (optionnel)
    4. RÃ©cupÃ©ration des cotes pour les matchs Ã  venir (optionnel)
    """
    supabase = get_client()
    if not supabase:
        print("âŒ Connexion Supabase impossible. VÃ©rifie .env (SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY).")
        sys.exit(1)

    try:
        from backend_engine import (
            ingest_recent_games,
            fetch_future_games,
            update_team_archetypes,
        )
    except ImportError as e:
        print(f"âŒ Import backend_engine: {e}")
        sys.exit(1)

    print("\n" + "=" * 50)
    print("ðŸ“¥ 01_ingest_data â€” L'Aspirateur")
    print("=" * 50)

    # Ã‰tape 1 : Scores passÃ©s + box_scores
    print("\nðŸ“… Ã‰tape 1 : RÃ©sultats des derniers jours + Box Scores")
    n_past = ingest_recent_games(days=days_past, max_games_per_run=max_games_past)
    print(f"   â†’ {n_past} match(s) ingÃ©rÃ©s (scores + box scores)")

    # Ã‰tape 2 : Calendrier Ã  venir (J+1 Ã  J+days_future)
    print(f"\nðŸ“… Ã‰tape 2 : Calendrier J+1 Ã  J+{days_future} (matchs Ã  venir)")
    n_future = fetch_future_games(days=days_future, max_games=max_games_future)
    print(f"   â†’ {n_future} match(s) Ã  venir ajoutÃ©s au calendrier")

    # Ã‰tape 3 : ArchÃ©types
    if not skip_archetypes:
        print("\nðŸ”„ Ã‰tape 3 : Mise Ã  jour des archÃ©types")
        update_team_archetypes()
    else:
        print("\nâ­ï¸ Ã‰tape 3 : ArchÃ©types ignorÃ©s (--skip-archetypes)")

    # Ã‰tape 4 : Cotes pour les matchs Ã  venir
    if fetch_odds:
        print("\nðŸ“Š Ã‰tape 4 : RÃ©cupÃ©ration des cotes (Odds) pour les matchs Ã  venir")
        n_odds = update_future_games_odds(supabase, max_games=max_games_future)
        print(f"   â†’ {n_odds} match(s) avec cotes mises Ã  jour (games_history.home_odd / away_odd)")
    else:
        print("\nâ­ï¸ Ã‰tape 4 : Cotes ignorÃ©es (--no-odds)")

    print("\nâœ… 01_ingest_data terminÃ©.")
    print("   Look-Ahead : Train = matchs avec scores (passÃ©) | Predict = matchs sans scores (futur)\n")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Ingestion donnÃ©es basketball + cotes â†’ Supabase (Sniper ETL)"
    )
    parser.add_argument(
        "--days",
        type=int,
        default=None,
        metavar="N",
        help="Raccourci : nombre de jours passÃ©s pour les scores (ex: --days 5 = 5 derniers jours). Prioritaire sur --days-past.",
    )
    parser.add_argument(
        "--days-past",
        type=int,
        default=1,
        help="Jours passÃ©s pour rÃ©cupÃ©rer les scores (dÃ©faut: 1 = veille)",
    )
    parser.add_argument(
        "--days-future",
        type=int,
        default=3,
        help="Jours Ã  venir pour le calendrier (dÃ©faut: 3)",
    )
    parser.add_argument(
        "--max-games-past",
        type=int,
        default=50,
        help="Nombre max de matchs Ã  ingÃ©rer dans le passÃ©",
    )
    parser.add_argument(
        "--max-games-future",
        type=int,
        default=150,
        help="Nombre max de matchs Ã  venir Ã  ajouter",
    )
    parser.add_argument(
        "--no-odds",
        action="store_true",
        help="Ne pas rÃ©cupÃ©rer les cotes API pour les matchs Ã  venir",
    )
    parser.add_argument(
        "--skip-archetypes",
        action="store_true",
        help="Ne pas mettre Ã  jour les archÃ©types Ã©quipes",
    )
    parser.add_argument(
        "--init-leagues",
        type=str,
        default=None,
        metavar="ID1,ID2",
        help="Initialisation Ligue : backfill massif (saisons 2023-2024, 2024-2025, FT + box_scores) pour les IDs donnÃ©s. Ex: --init-leagues 121,16 (EuroCup + BCL). N'exÃ©cute pas le pipeline daily.",
    )
    args = parser.parse_args()

    # Mode backfill (Initialisation Ligue) : uniquement les ligues indiquÃ©es, 2 derniÃ¨res saisons
    if args.init_leagues is not None:
        try:
            league_ids = [int(x.strip()) for x in args.init_leagues.split(",") if x.strip()]
        except ValueError:
            print("âŒ --init-leagues : liste d'IDs entiers sÃ©parÃ©s par des virgules (ex: 121,16)")
            sys.exit(1)
        if not league_ids:
            print("âŒ --init-leagues : au moins un league_id requis (ex: 121,16)")
            sys.exit(1)
        supabase = get_client()
        if not supabase:
            print("âŒ Connexion Supabase impossible.")
            sys.exit(1)
        try:
            from backend_engine import backfill_league_seasons
        except ImportError as e:
            print(f"âŒ Import backend_engine: {e}")
            sys.exit(1)
        print("\n" + "=" * 50)
        print("ðŸ“¥ 01_ingest_data â€” Initialisation Ligue (Backfill)")
        print("=" * 50)
        print(f"   Ligues : {league_ids} | Saisons : 2023-2024, 2024-2025")
        print("   RÃ©cupÃ©ration : tous les matchs FT + box_scores â†’ games_history, box_scores (UPSERT)\n")
        backfill_league_seasons(league_ids=league_ids, seasons=["2023-2024", "2024-2025"])
        print("\nâœ… Backfill terminÃ©. Lance 02_train_models.py pour rÃ©-entraÃ®ner le modÃ¨le.\n")
        return

    days_past = args.days if args.days is not None else args.days_past
    # En mode Deep Fetch (plus d'un jour), augmenter la cap pour ne rien rater
    max_games_past = max(args.max_games_past, days_past * 30) if days_past > 1 else args.max_games_past

    run_ingest(
        days_past=days_past,
        days_future=args.days_future,
        max_games_past=max_games_past,
        max_games_future=args.max_games_future,
        fetch_odds=not args.no_odds,
        skip_archetypes=args.skip_archetypes,
    )


if __name__ == "__main__":
    main()
